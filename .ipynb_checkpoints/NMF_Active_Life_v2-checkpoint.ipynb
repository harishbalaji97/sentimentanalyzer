{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92f5d648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 2] No such\n",
      "[nltk_data]     file or directory>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##BASIC PREPROCESS RELATED LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "##NLP LIBRARIES - NLTK USED IN BUILDING NLP PIPE LINE\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import urllib.request as url\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import requests\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ad5f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d200d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TEXT CLEANING\n",
    "##SCRUB WORD FUNCTION REMOVES HTML MARK UPS,SPECIAL CHARACTERS,NON ASCIII CHARACTERS,NON BREAKING SPACES ETC\n",
    "def scrub_words(outtext):\n",
    "    # Replace \\xao characters in text -\n",
    "    # \\xa0 is actually non-breaking space in Latin1 (ISO 8859-1), also chr(160).  \n",
    "    outtext = str(outtext)\n",
    "    outtext = re.sub('\\xa0', ' ', outtext)\n",
    "    outtext = re.sub(\"(\\\\W|\\\\d)\", ' ', outtext) # Replace non ascii and digits\n",
    "    outtext = re.sub('\\n(\\w*?)[\\s]', '', outtext)    # Replace new line characters and following text untill space\n",
    "    outtext = re.sub(\"<.*?>\", ' ', outtext)    # Remove html markup\n",
    "    outtext = re.sub(r'\\\\xa0',' ',outtext)    #outtext = re.sub('\\n',' ',outtext)\n",
    "    outtext = re.sub(r'\\\\n',' ',outtext)    #outtext = re.sub(r'\\xa0',' ',outtext)\n",
    "    outtext = re.sub(r'_',' ',outtext)\n",
    "    outtext = re.sub(r'  ',' ',outtext)\n",
    "    outtext = re.sub('[^a-zA-z\\s]','',outtext)\n",
    "    outtext = re.sub(' +', ' ',outtext)\n",
    "    \" \".join(outtext.strip())\n",
    "    return outtext\n",
    "  \n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode(\n",
    "        'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def text_cleaning(scrapped_raw_text):   \n",
    "    cleaned_text = scrub_words(scrapped_raw_text)\n",
    "    return cleaned_text\n",
    "  \n",
    "def text_processing(text):     \n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [token for token in word_tokenize(text) if not token in stop_words]\n",
    "    tokens = [token for token in word_tokenize(text) if len(token)>2]\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    ##LEMMATIZING WORDS\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [token for token in word_tokenize(text) if len(token)>2]\n",
    "    text = ' '.join(tokens)\n",
    "    return text   \n",
    "\n",
    "def Topic_Modeling(Test):\n",
    "    #Data Collection\n",
    "    df = pd.read_csv(\"Train_V1_yelp\")\n",
    "    #Test=pd.read_csv(\"Yelp_Test.csv\")\n",
    "    df[\"Industry\"].unique(),df[\"Categories_1\"].unique()\n",
    "    df = df.loc[df[\"Industry\"]==\"Active Life\"]\n",
    "    df= df.loc[df[\"Categories_1\"]!=\"Shopping\"]\n",
    "    df['Text'] = df['Text'].apply(str)\n",
    "    df[\"label\"] = df[\"stars\"].apply(lambda x : \"Negative\" if 1<=x<=2 else (\"Neutral\" if x==3 else \"Positive\") )\n",
    "    y = LabelEncoder().fit_transform(df['label'])\n",
    "\n",
    "    #Remove Stop words \n",
    "    stop_words = stopwords.words('english')\n",
    "    df['clean_text'] = df['Text'].apply(text_cleaning)\n",
    "    df['clean_text'] = df['clean_text'].apply(text_processing)\n",
    "    #Train data\n",
    "    X_train=df.reset_index().copy()\n",
    "    X_train.drop(columns=['Unnamed: 0','index'],inplace=True)\n",
    "    X_train.reset_index(inplace=True)\n",
    "    X_train.rename(columns={'index':'Id'},inplace=True)\n",
    "\n",
    "    #Modeling\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_df=0.9, max_features=1000, lowercase=False, ngram_range=(1,3))\n",
    "    tfidf_vectors = vectorizer.fit_transform(X_train.clean_text)\n",
    "    clf = decomposition.NMF(n_components=5, random_state=111,init=None,max_iter=10)\n",
    "    W1 = clf.fit_transform(tfidf_vectors)\n",
    "    H1 = clf.components_\n",
    "\n",
    "    #Topic Extraction\n",
    "    num_words=40\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in H1])\n",
    "    topics = [' '.join(t) for t in topic_words]\n",
    "\n",
    "    #Dominant topic for data\n",
    "    colnames = [\"Topic\" + str(i) for i in range(clf.n_components)]\n",
    "    docnames = [str(i) for i in range(len(X_train.clean_text))]\n",
    "    df_doc_topic = pd.DataFrame(np.round(W1, 2), columns=colnames, index=docnames)\n",
    "    significant_topic = np.argmax(df_doc_topic.values, axis=1)\n",
    "    df_doc_topic['dominant_topic'] = significant_topic\n",
    "\n",
    "    #Mapping dominant topic,Topics names to main data frame\n",
    "    Tags=pd.read_excel(\"Tags.xlsx\")\n",
    "    Topics_Tags=pd.DataFrame.from_dict({'Id':Tags['Id'],'Topics':Tags['Topics'],'Tags':Tags['Tags']})\n",
    "    doc_topic_train=pd.DataFrame.from_dict({'dominant_topic':df_doc_topic['dominant_topic']})\n",
    "    Train_Tag=pd.merge(left=doc_topic_train,right=Topics_Tags,left_on='dominant_topic',right_on='Id',how='left')\n",
    "    Train_Tag.reset_index(level=0, inplace=True)\n",
    "    Train_Tag.rename(columns={'index':'Tag_Id'},inplace=True)\n",
    "    Train_Tag=Train_Tag[['Tag_Id','Topics', 'Tags']]\n",
    "    Train_Data=pd.merge(left=X_train,right=Train_Tag,left_on='Id',right_on='Tag_Id',how='left')\n",
    "    Train_Data.drop(columns=['Tag_Id'],inplace=True)\n",
    "\n",
    "    #Testing Data\n",
    "    # Test modeling\n",
    "    WHold = clf.transform(vectorizer.transform(Test.Text))\n",
    "    #dominant topic\n",
    "    colnames = [\"Topic\" + str(i) for i in range(clf.n_components)]\n",
    "    docnames = [str(i) for i in range(len(Test.Text))]\n",
    "    df_doc_topic_test = pd.DataFrame(np.round(WHold, 2), columns=colnames, index=docnames)\n",
    "    significant_topic = np.argmax(df_doc_topic_test.values, axis=1)\n",
    "    df_doc_topic_test['dominant_topic'] = significant_topic\n",
    "    #Mapping\n",
    "    doc_topic=pd.DataFrame.from_dict({'dominant_topic':df_doc_topic_test['dominant_topic']})\n",
    "    Final_Tag=pd.merge(left=doc_topic,right=Topics_Tags,left_on='dominant_topic',right_on='Id',how='left')\n",
    "    Final_Tag.reset_index(level=0, inplace=True)\n",
    "    Final_Tag.rename(columns={'index':'Tag_Id'},inplace=True)\n",
    "    Final_Tag_v1=Final_Tag[['Tag_Id','Topics', 'Tags']]\n",
    "    #Final Data Frame\n",
    "    Modeling_Data=pd.merge(left=Test,right=Final_Tag_v1,left_on='Id',right_on='Tag_Id',how='left')\n",
    "    Modeling_Data=Modeling_Data.drop(columns=['Tag_Id'])\n",
    "    return Modeling_Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeed8699",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "text_processing() missing 1 required positional argument: 'stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\HARISH~1.SAM\\AppData\\Local\\Temp/ipykernel_17612/1001122160.py\u001b[0m in \u001b[0;36mTopic_Modeling\u001b[1;34m(Test)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_cleaning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_processing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;31m#Train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: text_processing() missing 1 required positional argument: 'stop_words'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test=Topic_Modeling(pd.read_csv(\"fgk.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a4e8f7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Date</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sender_Email_Type</th>\n",
       "      <th>Receiver_Email_Type</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>laxmi.saoji@gmail.com</td>\n",
       "      <td>ygayathri95@yahoo.com</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>Golf</td>\n",
       "      <td>Although I've never played this course, I give...</td>\n",
       "      <td>Gmail</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>course golf play The played courses greens hol...</td>\n",
       "      <td>Golf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>laxmi.saoji@gmail.com</td>\n",
       "      <td>ygayathri95@yahoo.com</td>\n",
       "      <td>2022-01-26</td>\n",
       "      <td>Golf</td>\n",
       "      <td>Easily the worse course I have played in Phoen...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>course golf play The played courses greens hol...</td>\n",
       "      <td>Golf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>laxmi.saoji@gmail.com</td>\n",
       "      <td>ygayathri95@yahoo.com</td>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>Golf</td>\n",
       "      <td>Played here on a Friday afternoon. The people ...</td>\n",
       "      <td>Outlook</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>course golf play The played courses greens hol...</td>\n",
       "      <td>Golf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>laxmi.saoji@gmail.com</td>\n",
       "      <td>ygayathri95@yahoo.com</td>\n",
       "      <td>2022-01-27</td>\n",
       "      <td>Golf</td>\n",
       "      <td>Golf course is my favorite in the east valley....</td>\n",
       "      <td>Outlook</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>course golf play The played courses greens hol...</td>\n",
       "      <td>Golf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>laxmi.saoji@gmail.com</td>\n",
       "      <td>ygayathri95@yahoo.com</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>Golf</td>\n",
       "      <td>I went out last saturday and had an good exper...</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>course golf play The played courses greens hol...</td>\n",
       "      <td>Golf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>laxmi.saoji@gmail.com</td>\n",
       "      <td>ygayathri95@yahoo.com</td>\n",
       "      <td>2022-01-09</td>\n",
       "      <td>Yoga</td>\n",
       "      <td>I LOVE Metta yoga. I moved here about a year a...</td>\n",
       "      <td>Outlook</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>yoga studio class classes Yoga instructors Bik...</td>\n",
       "      <td>Yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>laxmi.saoji@gmail.com</td>\n",
       "      <td>ygayathri95@yahoo.com</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>Yoga</td>\n",
       "      <td>Huge fan of Bikram yoga! It's a 90 minute, tor...</td>\n",
       "      <td>Outlook</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>yoga studio class classes Yoga instructors Bik...</td>\n",
       "      <td>Yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>laxmi.saoji@gmail.com</td>\n",
       "      <td>ygayathri95@yahoo.com</td>\n",
       "      <td>2022-01-08</td>\n",
       "      <td>Yoga</td>\n",
       "      <td>At the suggestion of some biker friends, I too...</td>\n",
       "      <td>Outlook</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>bike service would back shop said time get cus...</td>\n",
       "      <td>Sporting Goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>laxmi.saoji@gmail.com</td>\n",
       "      <td>ygayathri95@yahoo.com</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>Yoga</td>\n",
       "      <td>Because of my buisness I am lucky enough to tr...</td>\n",
       "      <td>Gmail</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>yoga studio class classes Yoga instructors Bik...</td>\n",
       "      <td>Yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>laxmi.saoji@gmail.com</td>\n",
       "      <td>ygayathri95@yahoo.com</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>Yoga</td>\n",
       "      <td>Sumits KILLS!!!\\n\\n\\n...but in a good way, hot...</td>\n",
       "      <td>Outlook</td>\n",
       "      <td>yahoo</td>\n",
       "      <td>yoga studio class classes Yoga instructors Bik...</td>\n",
       "      <td>Yoga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Id                   From                     To        Date  \\\n",
       "0            0   0  laxmi.saoji@gmail.com  ygayathri95@yahoo.com  2022-02-10   \n",
       "1            1   1  laxmi.saoji@gmail.com  ygayathri95@yahoo.com  2022-01-26   \n",
       "2            2   2  laxmi.saoji@gmail.com  ygayathri95@yahoo.com  2022-01-15   \n",
       "3            3   3  laxmi.saoji@gmail.com  ygayathri95@yahoo.com  2022-01-27   \n",
       "4            4   4  laxmi.saoji@gmail.com  ygayathri95@yahoo.com  2022-01-28   \n",
       "..         ...  ..                    ...                    ...         ...   \n",
       "95          95  95  laxmi.saoji@gmail.com  ygayathri95@yahoo.com  2022-01-09   \n",
       "96          96  96  laxmi.saoji@gmail.com  ygayathri95@yahoo.com  2022-01-03   \n",
       "97          97  97  laxmi.saoji@gmail.com  ygayathri95@yahoo.com  2022-01-08   \n",
       "98          98  98  laxmi.saoji@gmail.com  ygayathri95@yahoo.com  2022-02-02   \n",
       "99          99  99  laxmi.saoji@gmail.com  ygayathri95@yahoo.com  2022-01-02   \n",
       "\n",
       "   Subject                                               Text  \\\n",
       "0     Golf  Although I've never played this course, I give...   \n",
       "1     Golf  Easily the worse course I have played in Phoen...   \n",
       "2     Golf  Played here on a Friday afternoon. The people ...   \n",
       "3     Golf  Golf course is my favorite in the east valley....   \n",
       "4     Golf  I went out last saturday and had an good exper...   \n",
       "..     ...                                                ...   \n",
       "95    Yoga  I LOVE Metta yoga. I moved here about a year a...   \n",
       "96    Yoga  Huge fan of Bikram yoga! It's a 90 minute, tor...   \n",
       "97    Yoga  At the suggestion of some biker friends, I too...   \n",
       "98    Yoga  Because of my buisness I am lucky enough to tr...   \n",
       "99    Yoga  Sumits KILLS!!!\\n\\n\\n...but in a good way, hot...   \n",
       "\n",
       "   Sender_Email_Type Receiver_Email_Type  \\\n",
       "0              Gmail               yahoo   \n",
       "1              Yahoo               yahoo   \n",
       "2            Outlook               yahoo   \n",
       "3            Outlook               yahoo   \n",
       "4              Yahoo               yahoo   \n",
       "..               ...                 ...   \n",
       "95           Outlook               yahoo   \n",
       "96           Outlook               yahoo   \n",
       "97           Outlook               yahoo   \n",
       "98             Gmail               yahoo   \n",
       "99           Outlook               yahoo   \n",
       "\n",
       "                                               Topics            Tags  \n",
       "0   course golf play The played courses greens hol...            Golf  \n",
       "1   course golf play The played courses greens hol...            Golf  \n",
       "2   course golf play The played courses greens hol...            Golf  \n",
       "3   course golf play The played courses greens hol...            Golf  \n",
       "4   course golf play The played courses greens hol...            Golf  \n",
       "..                                                ...             ...  \n",
       "95  yoga studio class classes Yoga instructors Bik...            Yoga  \n",
       "96  yoga studio class classes Yoga instructors Bik...            Yoga  \n",
       "97  bike service would back shop said time get cus...  Sporting Goods  \n",
       "98  yoga studio class classes Yoga instructors Bik...            Yoga  \n",
       "99  yoga studio class classes Yoga instructors Bik...            Yoga  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32425d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
